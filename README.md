# Parameter-Efficient Fine-Tuning of Large Language Models

This repository contains the code, results, and report for my M.Tech thesis:
**"Parameter-Efficient Fine-Tuning Techniques on Large Language Models"**,
conducted at IIT Kharagpur under the guidance of Prof. Pawan Goyal and Prof. Arijit De.

## ðŸ“„ Overview
The project systematically evaluates multiple Parameter-Efficient Fine-Tuning (PEFT) methods
â€”including LoRA, QLoRA, AdaLoRA, LoHa, LoKr, IAÂ³, Prefix Tuning, and P-Tuning v2â€”
on two major NLP tasks:
1. **Summarization** (BillSum dataset) using FLAN-T5-XL and LLaMA-3.2-3B
2. **Classification** (dair-ai/emotion dataset)

## ðŸš€ Models & Hugging Face Links
| Task            | Model               | PEFT Method | Link |
|-----------------|--------------------|-------------|------|
| Summarization   | FLAN-T5-XL          | AdaLoRA     | [HF Link](https://huggingface.co/...) |
| Summarization   | LLaMA-3.2-3B        | LoKR        | [HF Link](https://huggingface.co/...) |
| Classification  | FLAN-T5-XL          | IAÂ³         | [HF Link](https://huggingface.co/...) |
| Classification  | LLaMA-3.2-3B        | LoRA        | [HF Link](https://huggingface.co/...) |

> All other PEFT variants and configurations are listed in `results/`.

## ðŸ“Š Key Results
- **Summarization:** AdaLoRA on T5 achieved ROUGE-1 of **50.54**, training only **0.2478%** of parameters.
- **Classification:** LoRA on LLaMA achieved an F1 score of **0.93**.
- **Efficiency:** QLoRA reduced GPU usage to ~6GB with minimal performance drop.

## ðŸ“‚ Repository Structure
```
.
â”œâ”€â”€ data/                  # Dataset preprocessing scripts
â”œâ”€â”€ notebooks/             # Jupyter notebooks for experiments
â”œâ”€â”€ src/                   # Training & evaluation scripts
â”œâ”€â”€ results/               # Experimental results & plots
â”œâ”€â”€ report/                # Full thesis PDF & figures
â”œâ”€â”€ scripts/               # Helper scripts
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md
```

## ðŸ“¦ Installation
```bash
git clone https://github.com/<your-username>/parameter-efficient-llm-finetuning.git
cd parameter-efficient-llm-finetuning
pip install -r requirements.txt
```

## ðŸ›  Usage
### Summarization
```bash
python src/summarization/train_t5.py --config configs/t5_lora.json
```

### Classification
```bash
python src/classification/train_llama.py --config configs/llama_adalora.json
```

## ðŸ“‘ Citation
If you use this work, please cite:
```
Beerukuri, Abhinay. "Parameter-Efficient Fine-Tuning Techniques on Large Language Models." M.Tech Thesis, IIT Kharagpur, 2025.
```

## ðŸ“œ License
This repository is licensed under the MIT License.
